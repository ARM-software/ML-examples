{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbfsgUcBG_hx"
   },
   "source": [
    "# Train and Deploy your NPU-enabled models\n",
    "\n",
    "> Using the Arm Corstone-300 with Cortex-M55 and Ethos-U55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jty-u4JWHViE"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook presents a flow to help bridge the gap between data scientists and embedded engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOvd37uuHTE7"
   },
   "source": [
    "## Training a Model\n",
    "\n",
    "In this example we are going to train a \"toy\" model. We will create a basic convolutional neural network model to solve the MNIST problem.\n",
    "\n",
    "The [MNIST database](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits which can be used to train a digit classifier. It is often used as a starter dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EatvozlIIlO"
   },
   "source": [
    "Let's start of by importing the required Python dependencies. For this we will use the [TensorFlow](https://github.com/tensorflow/tensorflow) framework for the model and [TensorFlow Datasets](https://github.com/tensorflow/datasets) to download the MNIST dataset. If you're using Google Colab, these dependencies come preinstalled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3CjmqOzKBY5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJFi-du1IGLt"
   },
   "source": [
    "We can now download the MNIST dataset using TensorFlow datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222,
     "referenced_widgets": [
      "20200e22b37c40f38d0cae9d63828d61",
      "d0332e909a1e41d4b79c78d872426973",
      "a866c812aefb42709c286b504c4b502a",
      "cc2f6b02e2fa4e6b9ba5cadb7c940e38",
      "bedb6318f10e47768eed2b0a76f80169",
      "07a92d5a2ba94fbfa98f8e69e858d1b7",
      "2e5a1d3859f74f7ebf8e3a0436276e4f",
      "b3d302a7846a4ebe8540ad07331ee1b5"
     ]
    },
    "id": "x1LDNN4CKJNJ",
    "outputId": "a9197162-c6d1-49c5-9741-322234aae8dd"
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, \n",
    "  as_supervised=True, with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv_Oq-qbekcD"
   },
   "source": [
    "Once downloaded, we write a function to preprocess the MNIST dataset ready for use in a neural network. The images come in `uint8` format, and so to normalize the dataset so that all values are between `[0, 1]` we divde by `255` (the max `uint8` value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2WykZXvKMcB"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOHsvHJFfBZK"
   },
   "source": [
    "Let's apply this function to the dataset using `.map` and take a batch size of `128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9ifl7ztLp-K"
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(\n",
    "  normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(\n",
    "  normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0CIYup1fP3j"
   },
   "source": [
    "We are now ready to create the model using the `Sequential` functionality. \n",
    "\n",
    "Although we could achieve a model with high accuracy using a fully connected model, this would require a lot of weights and biases. The Ethos-U55 is designed to be used with a Cortex-M55 meaning there will be memory limits. For this reason we build a convolutional network with large kernel sizes to reduce the number of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXs4ohIBLtJs"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28,28,1)),\n",
    "  tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC85LSpgcW5"
   },
   "source": [
    "We are now ready to train the model. For this toy example we will just train for a singular epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hQWCshvgh3X",
    "outputId": "81d893d9-a1cf-445d-d86c-062558279b79"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(0.001), \n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(ds_train, epochs=1, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWWHoEwEgrgn"
   },
   "source": [
    "## Quantize the Model\n",
    "\n",
    "The next step is to quantize the model. This converts the weights from floating-point numbers to integer numbers. The Ethos-U55 supports 8 bit weights, and 8 bit and 16 bit activations. \n",
    "\n",
    "In this example we will quantize the model into `int8` format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Itj03AI3g-3u"
   },
   "source": [
    "Let's first `unbatch` the dataset from 128 samples at a time. In inference we will only be running one image at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6mQX5slMVRi"
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train.unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F16ebTLfhIbS"
   },
   "source": [
    "We can then build a generator function to use in the conversion process. \n",
    "\n",
    "Creating a generator allows the TensorFlow Lite converter find the best weights to fall to based on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK0O6BZxhHvW"
   },
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value, output_value in ds_train.batch(1).take(100):\n",
    "    yield [input_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cu6-dIGhUOR"
   },
   "source": [
    "Finally we are ready to convert the model. We can use the `from_keras_model` method to create a converter from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G74vW7fHhWwO"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cseaAGpahiGw"
   },
   "source": [
    "We can then set the `inference_input_type`, `inference_output_type` and `supported_ops` to `int8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfERi-ZihhnQ"
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFkw1yKeh9MR"
   },
   "source": [
    "We then add the `representative_dataset` to be our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51eB1Qcph5fP"
   },
   "outputs": [],
   "source": [
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ9plu3riC8S"
   },
   "source": [
    "The last step is to run the conversion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dJDoeRYiDXW",
    "outputId": "d3f6682c-ea18-4d87-fcba-907b48b91d82"
   },
   "outputs": [],
   "source": [
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G02dEuPeiG87"
   },
   "source": [
    "We now have a quantized model in TFLite format. Lets save this to our files as `my_model.tflite`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0fUWRdtNd2S"
   },
   "outputs": [],
   "source": [
    "with open(\"my_model.tflite\", \"wb\") as f:\n",
    "  f.write(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "freVF5X5iZOm"
   },
   "source": [
    "## Vela Compiler\n",
    "\n",
    "When creating a model for use on Ethos-U55 we need to use the Vela Compiler to optimise the model.\n",
    "\n",
    "This is a command-line tool written in Python which takes a `.tflite` file and outputs another `.tflite` file. The new file is restructured in a way that Ethos-U understands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSWWZxa3jHqN"
   },
   "source": [
    "To do this, lets first install `ethos-u-vela` for the compiler and `xxd` which will be used to convert binary files into hexdumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhxBhPy2Lw_e",
    "outputId": "6d794fa9-bde1-4176-dde5-aa342a04d672"
   },
   "outputs": [],
   "source": [
    "!pip install ethos-u-vela\n",
    "!apt install -y xxd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIYnCP6FjkRL"
   },
   "source": [
    "We can now compile the model. For this we will specify the config as `ethos-u55-128`. This is one of the commonly used templates for Ethos-U55. This configuration has 128 macs. We will create a `vela.ini` file with our system configuration description. This information helps vela to optimize model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vela.ini\n",
    "\n",
    "[System_Config.Ethos_U55_High_End_Embedded]\n",
    "core_clock=500e6\n",
    "axi0_port=Sram\n",
    "axi1_port=OffChipFlash\n",
    "Sram_clock_scale=1.0\n",
    "Sram_burst_length=32\n",
    "Sram_read_latency=32\n",
    "Sram_write_latency=32\n",
    "OffChipFlash_clock_scale=0.125\n",
    "OffChipFlash_burst_length=128\n",
    "OffChipFlash_read_latency=64\n",
    "OffChipFlash_write_latency=64\n",
    "\n",
    "; Shared SRAM: the SRAM is shared between the Ethos-U and the Cortex-M software\n",
    "; The non-SRAM memory is assumed to be read-only\n",
    "[Memory_Mode.Shared_Sram]\n",
    "const_mem_area=Axi1\n",
    "arena_mem_area=Axi0\n",
    "cache_mem_area=Axi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfaE1o5VTLmm",
    "outputId": "32f7d41c-5bfa-4bef-bff4-3825d1da3ff1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "vela --accelerator-config=ethos-u55-128 \\\n",
    "--optimise Performance \\\n",
    "--memory-mode=Shared_Sram \\\n",
    "--system-config=Ethos_U55_High_End_Embedded \\\n",
    "--config vela.ini \\\n",
    "my_model.tflite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiNyZX9lj1Oe"
   },
   "source": [
    "We can then convert the `.tflite` binary into a hexdump C headerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f-vZlyhj0rm",
    "outputId": "e5f468d3-012a-4579-8c1e-d92654af5b5a"
   },
   "outputs": [],
   "source": [
    "!xxd -i output/my_model_vela.tflite my_network_model.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HceA9y8Oj_hF"
   },
   "source": [
    "The last step is to do some cleaning up of the file for the application. Here we rename the model from `output_my_model_vela_tflite` to `network_model` and add some header guards to the file.\n",
    "\n",
    "The most important is to add model variable attribute `__attribute__((aligned(16)))` for 16 bytes alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t2Y0Aapj_N3",
    "outputId": "8067d046-abb2-4230-c7b8-13f52322790a"
   },
   "outputs": [],
   "source": [
    "!sed -i 's/unsigned int output_my_model_vela_tflite_len/const unsigned int network_model_len/' my_network_model.h\n",
    "!sed -i 's/unsigned char output_my_model_vela_tflite\\[\\]/const unsigned char network_model\\[\\] __attribute__((aligned(16)))/' my_network_model.h\n",
    "\n",
    "!sed -i '1s/^/#define NETWORK_MODEL_H\\n/' my_network_model.h\n",
    "!sed -i '1s/^/#ifndef NETWORK_MODEL_H\\n/' my_network_model.h\n",
    "!echo \"#endif //NETWORK_MODEL_H\" >> my_network_model.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r93Zxt3rIrum"
   },
   "source": [
    "## Build the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uV9WkffSkwtK"
   },
   "source": [
    "With the model now ready to use in the application, we need to generate some test data to use in the model. To do this we create two functions, `write_input_headerfile` which writes an example input array to a headerfile and `write_output_headerfile` which writes the expected output array to the headerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYiiG831BwRh"
   },
   "outputs": [],
   "source": [
    "def write_input_headerfile(array):\n",
    "  with open(\"input_data.h\", \"w\") as f:\n",
    "    line = \"#ifndef INPUT_DATA_H\\n#define INPUT_DATA_H\\n\\n\"\n",
    "    f.write(line)\n",
    "    line = f\"static const int input_data_len = {len(array)};\\n\"\n",
    "    f.write(line)\n",
    "    line = \"static const int8_t input_data[] = {\\n  \"\n",
    "    f.write(line)\n",
    "    count = 0\n",
    "    for val in array:\n",
    "      if (count+1)%8 == 0:\n",
    "        line = f\"{val},\\n  \"\n",
    "      else:\n",
    "        line = f\"{val}, \"\n",
    "      count += 1\n",
    "      if count == len(array):\n",
    "        line = line.replace(\",\",\"\")\n",
    "      f.write(line)\n",
    "    line = \"\\n};\\n\\n\"\n",
    "    f.write(line)\n",
    "    line = \"#endif // INPUT_DATA_H\"\n",
    "    f.write(line)\n",
    "\n",
    "  return None\n",
    "\n",
    "def write_output_headerfile(array):\n",
    "  with open(\"expected_output_data.h\", \"w\") as f:\n",
    "    line = \"#ifndef EXPECTED_OUTPUT_DATA_H\\n#define EXPECTED_OUTPUT_DATA_H\\n\\n\"\n",
    "    f.write(line)\n",
    "    line = f\"static const int expected_output_data_len = {len(array)};\\n\"\n",
    "    f.write(line)\n",
    "    line = \"static const int8_t expected_output_data[] = {\\n  \"\n",
    "    f.write(line)\n",
    "    count = 0\n",
    "    for val in array:\n",
    "      if (count+1)%8 ==0:\n",
    "        line = f\"{val},\\n  \"\n",
    "      else:\n",
    "        line = f\"{val}, \"\n",
    "      count += 1\n",
    "      if count == len(array):\n",
    "        line = line.replace(\",\",\"\")\n",
    "      f.write(line)\n",
    "\n",
    "    line = \"\\n};\\n\\n\"\n",
    "    f.write(line)\n",
    "    line = \"#endif // EXPECTED_OUTPUT_DATA_H\"\n",
    "    f.write(line)\n",
    "\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8fTiaEQlGWu"
   },
   "source": [
    "Let's take an input from a test set for use in the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbE-ckxzTuiL"
   },
   "outputs": [],
   "source": [
    "# Load the model into tflite\n",
    "tflite_model = tf.lite.Interpreter(\"my_model.tflite\")\n",
    "\n",
    "# Get the input and output information from the model\n",
    "input_details = tflite_model.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "output_details = tflite_model.get_output_details()\n",
    "\n",
    "# Unbatch the test dataset\n",
    "ds_test = ds_test.unbatch()\n",
    "\n",
    "# Take one example from the test set\n",
    "for x,y in ds_test.batch(1).take(1):\n",
    "  # Convert the input to a numpy array\n",
    "  x_numpy = x.numpy()\n",
    "  # Quantize the input data into int8 format\n",
    "  x_numpy = x_numpy / input_scale + input_zero_point\n",
    "  x_numpy = x_numpy.astype(input_details[0][\"dtype\"])\n",
    "  # Write the array to a headerfile\n",
    "  write_input_headerfile(x_numpy.flatten())\n",
    "\n",
    "  # Run the model to get the expected output\n",
    "  tflite_model.allocate_tensors()\n",
    "  #tflite_model.set_tensor(input_details[0]['index'], np.expand_dims(x_numpy,axis=0))\n",
    "  tflite_model.set_tensor(input_details[0]['index'], x_numpy)\n",
    "  tflite_model.invoke()\n",
    "\n",
    "  # Get the output array from the model\n",
    "  output_data = tflite_model.get_tensor(output_details[0][\"index\"])\n",
    "  # Write the headerfile for the expected output\n",
    "  write_output_headerfile(output_data.flatten())\n",
    "\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aq7hrxnl4Tn"
   },
   "source": [
    "With everything now ready to build the application we can start downloading the dependencies for the application. We first download the `tflite-micro` repository and checkout specific tested commit. Then remove unnecessary default model download from the generation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zHVbrxgpQuD",
    "outputId": "2865db53-09f3-421d-c051-38883cb03c0f"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/tensorflow/tflite-micro.git\n",
    "cd tflite-micro\n",
    "git checkout 080db2db55e82863d1a9adcf5c045d48e3e73941\n",
    "\n",
    "sed -i '58,62d' tensorflow/lite/micro/tools/make/ext_libs/ethos_u.inc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7U1QGdEN53"
   },
   "source": [
    "Now we generate the TensorFlow Lite Micro (TFLite-Micro) library for use in our application. To do this we set the target as the `cortex_m_corstone_300` which has been built into TFLite-Micro, the architecture as `cortex-m55`, the `CO_PROCESSOR=ethos_u` tag enabled Ethos-U kernels in the library and finally we use `cmsis_nn` to provide optimized kernels for Cortex-M55:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZPMEMAvoRvI",
    "outputId": "b46a8c41-9c5d-45d4-dcdd-058732e7f465"
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "cd tflite-micro\n",
    "make -f tensorflow/lite/micro/tools/make/Makefile third_party_downloads\n",
    "make -f tensorflow/lite/micro/tools/make/Makefile \\\n",
    "TARGET=cortex_m_corstone_300 \\\n",
    "TARGET_ARCH=cortex-m55 \\\n",
    "CO_PROCESSOR=ethos_u \\\n",
    "OPTIMIZED_KERNEL_DIR=cmsis_nn \\\n",
    "microlite -j8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSLr2x6koeUn"
   },
   "source": [
    "We now have a compiled Tensorflow micro library file called `libtensorflow-microlite.a`.\n",
    "\n",
    "Lets now write the application using Tensorflow micro test framework. This is a testing application which will compare the output with the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAhQhxdYEuUg",
    "outputId": "c731d555-8f34-4f97-a9f6-68f2f74b3bc2"
   },
   "outputs": [],
   "source": [
    "%%writefile main.cc\n",
    "\n",
    "#include \"tensorflow/lite/micro/micro_error_reporter.h\"\n",
    "#include \"tensorflow/lite/micro/micro_interpreter.h\"\n",
    "#include \"tensorflow/lite/micro/micro_utils.h\"\n",
    "#include \"tensorflow/lite/micro/testing/micro_test.h\"\n",
    "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
    "#include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\n",
    "\n",
    "#include \"my_network_model.h\"\n",
    "#include \"input_data.h\"\n",
    "#include \"expected_output_data.h\"\n",
    "\n",
    "#define TENSOR_ARENA_SIZE (70 * 1024)\n",
    "\n",
    "uint8_t tensor_arena[TENSOR_ARENA_SIZE];\n",
    "\n",
    "TF_LITE_MICRO_TESTS_BEGIN\n",
    "\n",
    "TF_LITE_MICRO_TEST(TestInvoke) {\n",
    "  \n",
    "  tflite::MicroErrorReporter micro_error_reporter;\n",
    "  // load the model\n",
    "  const tflite::Model* model = ::tflite::GetModel(network_model);\n",
    "  if (model->version() != TFLITE_SCHEMA_VERSION) {\n",
    "    TF_LITE_REPORT_ERROR(&micro_error_reporter,\n",
    "                         \"Model provided is schema version %d not equal \"\n",
    "                         \"to supported version %d.\\n\",\n",
    "                         model->version(), TFLITE_SCHEMA_VERSION);\n",
    "    return kTfLiteError;\n",
    "  }\n",
    "\n",
    "  TF_LITE_REPORT_ERROR(&micro_error_reporter, \"Hello TFLITE Micro Tests.\\n\");\n",
    "  tflite::MicroMutableOpResolver<1> micro_op_resolver;\n",
    "  //tell tensorflow micro to add ethos-u operator   \n",
    "  micro_op_resolver.AddEthosU();\n",
    "\n",
    "  tflite::MicroInterpreter interpreter(\n",
    "      model, micro_op_resolver, tensor_arena, TENSOR_ARENA_SIZE, &micro_error_reporter);\n",
    "\n",
    "  TfLiteStatus allocate_status = interpreter.AllocateTensors();\n",
    "  if (allocate_status != kTfLiteOk) {\n",
    "    TF_LITE_REPORT_ERROR(&micro_error_reporter, \"Tensor allocation failed\\n\");\n",
    "    return kTfLiteError;\n",
    "  }\n",
    "\n",
    "  TfLiteTensor* input = interpreter.input(0);\n",
    "  TfLiteTensor* output = interpreter.output(0);\n",
    "\n",
    "  memcpy(input->data.int8, &input_data, input->bytes);\n",
    "\n",
    "  TfLiteStatus invoke_status = interpreter.Invoke();\n",
    "\n",
    "  if (invoke_status != kTfLiteOk) {\n",
    "    TF_LITE_REPORT_ERROR(&micro_error_reporter, \"Invoke failed\\n\");\n",
    "      return kTfLiteError;\n",
    "  }\n",
    "  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);\n",
    "\n",
    "  for (int i=0; i < expected_output_data_len; i++) {\n",
    "    TF_LITE_MICRO_EXPECT_EQ(output->data.int8[i], expected_output_data[i]);\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "TF_LITE_MICRO_TESTS_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHU49uFpp6ol"
   },
   "source": [
    "and the `Makefile` used to build the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rxhwbcsI3cN",
    "outputId": "f1eb0667-e04d-443d-c4e4-6d267bca6d15"
   },
   "outputs": [],
   "source": [
    "%%writefile Makefile\n",
    "\n",
    "TFLM_ROOT := tflite-micro/tensorflow/lite/micro\n",
    "THIRD_PARTY := $(TFLM_ROOT)/tools/make/downloads\n",
    "TARGET := cortex_m_corstone_300\n",
    "TARGET_CPU := cortex-m55\n",
    "\n",
    "\n",
    "CXX := $(TFLM_ROOT)/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-g++\n",
    "\n",
    "INCLUDES := -I$(TFLM_ROOT)/../../..\\\n",
    "  -I. \\\n",
    "  -I$(THIRD_PARTY)/flatbuffers/include \n",
    "\n",
    "\n",
    "CXXFLAGS := -std=c++11 -O3 -mfloat-abi=hard -mlittle-endian -MD -mcpu=cortex-m55 -mfpu=auto -mthumb \\\n",
    "    -Werror -Wvla -Wall -Wextra -funsigned-char -fno-function-sections \\\n",
    "    -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections \\\n",
    "    -fdata-sections -Wno-unused-parameter -DTF_LITE_STATIC_MEMORY \n",
    "\n",
    "\n",
    "OBJCOPY := $(TFLM_ROOT)/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-objcopy\n",
    "\n",
    "\n",
    "LDFLAGS := -lm\\\n",
    "  $(TFLM_ROOT)/tools/make/gen/$(TARGET)_$(TARGET_CPU)_default/lib/libtensorflow-microlite.a\\\n",
    "  -Wl,--fatal-warnings -Wl,--gc-sections --specs=nosys.specs -T \\\n",
    "\t$(THIRD_PARTY)/ethos_u_core_platform/targets/corstone-300/platform_parsed.ld \\\n",
    "\t-Wl,-Map=$(TFLM_ROOT)/tools/make/gen/cortex_m_corstone_300.map,--cref -lm \\\n",
    "\t-Wl,--gc-sections --entry Reset_Handler\n",
    "\n",
    "\n",
    "%.o:\t%.cc\n",
    "\t$(CXX) $(CXXFLAGS) $(INCLUDES) -c $< -o $@\n",
    "\n",
    "all:\tmain.bin\n",
    "\n",
    "all_objs := main.o\n",
    "\n",
    "main:\t$(all_objs)\n",
    "\t$(CXX) $(CXXFLAGS) $(INCLUDES) -o main $(all_objs) $(LDFLAGS)\n",
    "\n",
    "main.bin: main\n",
    "\t$(OBJCOPY) main main.bin -O binary\n",
    "\n",
    "clean:\n",
    "\t$(RM) *.o *.d *.map main.bin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b9oHFLlp_4j"
   },
   "source": [
    "Build the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkwM46k-Ozoh",
    "outputId": "8408711d-943d-4fb1-9bb2-536221abe8da"
   },
   "outputs": [],
   "source": [
    "!make clean\n",
    "!make -j8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5DoS1mDI1WI"
   },
   "source": [
    "## Deploy application on Corstone-300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhWea6KcqCvs"
   },
   "source": [
    "Run the application in Corstone-300:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEt0YJmYSot9",
    "outputId": "6a10748b-80e1-475c-f2af-a062ee315e69"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "FVP=\"tflite-micro/tensorflow/lite/micro/tools/make/downloads/corstone300/models/Linux64_GCC-6.4/FVP_Corstone_SSE-300_Ethos-U55 \"\n",
    "FVP+=\"-C mps3_board.visualisation.disable-visualisation=1 \"\n",
    "FVP+=\"-C mps3_board.telnetterminal0.start_telnet=0 \"\n",
    "FVP+='-C mps3_board.uart0.out_file=\"-\" '\n",
    "FVP+='-C mps3_board.uart0.unbuffered_output=1 '\n",
    "FVP+='-C mps3_board.uart0.shutdown_on_eot=1'\n",
    "\n",
    "$FVP main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_and_deploy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07a92d5a2ba94fbfa98f8e69e858d1b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20200e22b37c40f38d0cae9d63828d61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a866c812aefb42709c286b504c4b502a",
       "IPY_MODEL_cc2f6b02e2fa4e6b9ba5cadb7c940e38"
      ],
      "layout": "IPY_MODEL_d0332e909a1e41d4b79c78d872426973"
     }
    },
    "2e5a1d3859f74f7ebf8e3a0436276e4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a866c812aefb42709c286b504c4b502a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Dl Completed...: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07a92d5a2ba94fbfa98f8e69e858d1b7",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bedb6318f10e47768eed2b0a76f80169",
      "value": 4
     }
    },
    "b3d302a7846a4ebe8540ad07331ee1b5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bedb6318f10e47768eed2b0a76f80169": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cc2f6b02e2fa4e6b9ba5cadb7c940e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3d302a7846a4ebe8540ad07331ee1b5",
      "placeholder": "​",
      "style": "IPY_MODEL_2e5a1d3859f74f7ebf8e3a0436276e4f",
      "value": " 4/4 [00:00&lt;00:00,  4.40 file/s]"
     }
    },
    "d0332e909a1e41d4b79c78d872426973": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
