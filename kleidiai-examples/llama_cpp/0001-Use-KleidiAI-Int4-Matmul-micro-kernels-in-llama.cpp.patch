From 5dcac94d697cdb2554e3afa43c5643a9509b5224 Mon Sep 17 00:00:00 2001
From: Gian Marco Iodice <gianmarco.iodice@arm.com>
Date: Mon, 18 Nov 2024 16:57:08 +0000
Subject: [PATCH] Use KleidiAI Int4 Matmul micro-kernels in llama.cpp
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

- Update CMake file to fetch the Int4 micro-kernels from the KleidiAI
  repository
- Rebase to master@b8deef0ec0af5febac1d2cfd9119ff330ed0b762
- Add runtime CPU feature detection (Arm® Neon™, dotprod, i8mm and SME)
- Add weight caching feature for KleidiAI
- Adds SME kernel support from KleidiAI

Signed-off-by: Gian Marco Iodice <gianmarco.iodice@arm.com>
---
 ggml/CMakeLists.txt        |   2 +
 ggml/include/ggml-cpu.h    |  13 +
 ggml/src/CMakeLists.txt    |  65 +++
 ggml/src/ggml-alloc.c      |  13 +
 ggml/src/ggml-cpu.c        |  37 +-
 ggml/src/ggml-kleidiai.cpp | 930 +++++++++++++++++++++++++++++++++++++
 ggml/src/ggml-kleidiai.h   |  45 ++
 ggml/src/ggml.c            |  13 +
 src/CMakeLists.txt         |   4 +
 src/llama.cpp              |  15 +-
 10 files changed, 1122 insertions(+), 15 deletions(-)
 create mode 100644 ggml/src/ggml-kleidiai.cpp
 create mode 100644 ggml/src/ggml-kleidiai.h

diff --git a/ggml/CMakeLists.txt b/ggml/CMakeLists.txt
index cfa6e3f7..9713046a 100644
--- a/ggml/CMakeLists.txt
+++ b/ggml/CMakeLists.txt
@@ -141,6 +141,8 @@ option(GGML_CUDA_NO_VMM                     "ggml: do not try to use CUDA VMM"
 option(GGML_CUDA_FA_ALL_QUANTS              "ggml: compile all quants for FlashAttention"     OFF)
 option(GGML_CUDA_GRAPHS                     "ggml: use CUDA graphs (llama.cpp only)"          ${GGML_CUDA_GRAPHS_DEFAULT})
 
+option(GGML_KLEIDIAI                        "ggml: use KleidiAI"                              ON)
+
 option(GGML_HIPBLAS                         "ggml: use hipBLAS"                               OFF)
 option(GGML_HIP_UMA                         "ggml: use HIP unified memory architecture"       OFF)
 option(GGML_VULKAN                          "ggml: use Vulkan"                                OFF)
diff --git a/ggml/include/ggml-cpu.h b/ggml/include/ggml-cpu.h
index 7f1ee757..4b9d0918 100644
--- a/ggml/include/ggml-cpu.h
+++ b/ggml/include/ggml-cpu.h
@@ -44,6 +44,17 @@ extern "C" {
         void *              abort_callback_data;
     };
 
+    struct ggml_compute_params {
+        // ith = thread index, nth = number of threads
+        int ith, nth;
+
+        // work buffer for all threads
+        size_t wsize;
+        void * wdata;
+
+        struct ggml_threadpool * threadpool;
+    };
+
     // numa strategies
     enum ggml_numa_strategy {
         GGML_NUMA_STRATEGY_DISABLED   = 0,
@@ -54,6 +65,8 @@ extern "C" {
         GGML_NUMA_STRATEGY_COUNT
     };
 
+    GGML_API void    ggml_barrier(struct ggml_threadpool * tp);
+
     GGML_API void    ggml_numa_init(enum ggml_numa_strategy numa); // call once for better performance on NUMA systems
     GGML_API bool    ggml_is_numa(void); // true if init detected that system has >1 NUMA node
 
diff --git a/ggml/src/CMakeLists.txt b/ggml/src/CMakeLists.txt
index 34b81bd7..c2f80603 100644
--- a/ggml/src/CMakeLists.txt
+++ b/ggml/src/CMakeLists.txt
@@ -630,6 +630,70 @@ if (GGML_RPC)
     set(GGML_SOURCES_RPC ggml-rpc.cpp)
 endif()
 
+if (GGML_KLEIDIAI)
+
+    # Disable the KleidiAI tests
+    set(KLEIDIAI_BUILD_TESTS  OFF)
+
+    # Fetch KleidiAI sources:
+    include(FetchContent)
+    set(KLEIDIAI_COMMIT_SHA "v0.5.0")
+    set(KLEIDIAI_DOWNLOAD_URL "https://gitlab.arm.com/kleidi/kleidiai/-/archive/${KLEIDIAI_COMMIT_SHA}/kleidiai-${KLEIDIAI_COMMIT_SHA}.tar.gz")
+    set(KLEIDIAI_ARCHIVE_MD5  "b75f2d3c1bd2a5457091cc3976e2fdde")
+
+    if (POLICY CMP0135)
+        cmake_policy(SET CMP0135 NEW)
+    endif()
+
+    FetchContent_Declare(KleidiAI_Download
+        URL ${KLEIDIAI_DOWNLOAD_URL}
+        DOWNLOAD_EXTRACT_TIMESTAMP NEW
+        URL_HASH MD5=${KLEIDIAI_ARCHIVE_MD5})
+
+    FetchContent_MakeAvailable(KleidiAI_Download)
+    FetchContent_GetProperties(KleidiAI_Download
+        SOURCE_DIR  KLEIDIAI_SRC
+        POPULATED   KLEIDIAI_POPULATED)
+
+    if (NOT KLEIDIAI_POPULATED)
+        message(FATAL_ERROR "KleidiAI source downloaded failed.")
+    endif()
+
+    list(APPEND GGML_SOURCES_KLEIDIAI ggml-kleidiai.cpp)
+    list(APPEND GGML_HEADERS_KLEIDIAI ggml-kleidiai.h)
+
+    # KleidiAI
+    include_directories(
+        ${KLEIDIAI_SRC}/
+        ${KLEIDIAI_SRC}/kai/
+        ${KLEIDIAI_SRC}/kai/ukernels/
+        ${KLEIDIAI_SRC}/kai/ukernels/matmul/
+        ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/
+        ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/)
+
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f32.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32ps1s0scalef16_qsu4c32s16s0_neon.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f32_neon.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa.c)
+    list(APPEND GGML_SOURCES_KLEIDIAI ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.c)
+    list(APPEND GGML_CDEF_PUBLIC GGML_USE_KLEIDIAI)
+
+    set_source_files_properties(${GGML_SOURCES_KLEIDIAI} PROPERTIES COMPILE_OPTIONS -march=armv8.2-a+i8mm+dotprod+sve+sve2+fp16)
+
+    add_compile_definitions(GGML_USE_KLEIDIAI)
+    add_compile_definitions(GGML_KLEIDIAI_REUSE_MEMORY)
+
+    if (GGML_KLEIDIAI_CACHE)
+        add_compile_definitions(GGML_KLEIDIAI_USE_CACHE)
+    endif()
+
+endif()
+
 if (GGML_VULKAN)
     find_package(Vulkan COMPONENTS glslc REQUIRED)
 
@@ -1388,6 +1452,7 @@ add_library(ggml
             ${GGML_SOURCES_LLAMAFILE} ${GGML_HEADERS_LLAMAFILE}
             ${GGML_SOURCES_AMX}       ${GGML_HEADERS_AMX}
             ${GGML_SOURCES_CANN}      ${GGML_HEADERS_CANN}
+            ${GGML_SOURCES_KLEIDIAI}  ${GGML_HEADERS_KLEIDIAI}
             ggml-aarch64.c            ggml-aarch64.h
             )
 
diff --git a/ggml/src/ggml-alloc.c b/ggml/src/ggml-alloc.c
index 041de9e3..7263d328 100644
--- a/ggml/src/ggml-alloc.c
+++ b/ggml/src/ggml-alloc.c
@@ -9,6 +9,10 @@
 #include <stdlib.h>
 #include <string.h>
 
+#if defined(GGML_USE_KLEIDIAI)
+#include "ggml-kleidiai.h"
+#endif
+
 #define MAX(a, b) ((a) > (b) ? (a) : (b))
 #define MAX_FREE_BLOCKS 256
 
@@ -985,6 +989,15 @@ ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_conte
         size_t this_size = 0;
         if (t->data == NULL && t->view_src == NULL) {
             this_size = GGML_PAD(ggml_backend_buft_get_alloc_size(buft, t), alignment);
+#if defined(GGML_USE_KLEIDIAI)
+            // Temporary solution to allocate more memory if needed for packing the weights.
+            // This method is not sufficient as we assume that the weights are for matmul only.
+            // However, weights could belong to other operations
+            const int64_t kai_diff = (ggml_kai_get_const_workspace_size_matmul(t) - this_size);
+            if (kai_diff > 0) {
+                this_size += kai_diff;
+            }
+#endif
         }
 
         if (this_size > max_size) {
diff --git a/ggml/src/ggml-cpu.c b/ggml/src/ggml-cpu.c
index 0cb5b824..8472dd87 100644
--- a/ggml/src/ggml-cpu.c
+++ b/ggml/src/ggml-cpu.c
@@ -33,6 +33,10 @@
 #include <syscall.h>
 #endif
 
+#if defined(GGML_USE_KLEIDIAI)
+#include "ggml-kleidiai.h"
+#endif
+
 #ifdef GGML_USE_OPENMP
 #include <omp.h>
 #endif
@@ -1335,17 +1339,6 @@ struct ggml_compute_state {
     int ith;
 };
 
-struct ggml_compute_params {
-    // ith = thread index, nth = number of threads
-    int ith, nth;
-
-    // work buffer for all threads
-    size_t wsize;
-    void * wdata;
-
-    struct ggml_threadpool * threadpool;
-};
-
 //
 // fundamental operations
 //
@@ -2265,7 +2258,7 @@ void ggml_critical_section_end(void) {
     atomic_flag_clear(&g_state_critical);
 }
 
-static void ggml_barrier(struct ggml_threadpool * tp) {
+void ggml_barrier(struct ggml_threadpool * tp) {
     int n_threads = atomic_load_explicit(&tp->n_threads_cur, memory_order_relaxed);
     if (n_threads == 1) {
         return;
@@ -12199,6 +12192,13 @@ static void ggml_compute_forward(struct ggml_compute_params * params, struct ggm
         return;
     }
 
+#ifdef GGML_USE_KLEIDIAI
+    bool can_use_kai = ggml_kai_compute_forward(params, tensor);
+    if (can_use_kai) {
+        return;
+    }
+#endif // GGML_USE_KLEIDIAI
+
     switch (tensor->op) {
         case GGML_OP_DUP:
             {
@@ -13173,6 +13173,13 @@ struct ggml_cplan ggml_graph_plan(
                 {
                     const enum ggml_type vec_dot_type = type_traits_cpu[node->src[0]->type].vec_dot_type;
 
+#if defined(GGML_USE_KLEIDIAI)
+                    if (ggml_kai_can_accelerate_matmul(node->src[0], node->src[1], node)) {
+                        cur = ggml_kai_get_temp_workspace_size_matmul(node->src[0], node->src[1], node);
+                        break;
+                    }
+#endif
+
                     if (node->src[1]->type != vec_dot_type) {
                         cur = ggml_row_size(vec_dot_type, ggml_nelements(node->src[1]));
                     }
@@ -13574,6 +13581,12 @@ enum ggml_status ggml_graph_compute(struct ggml_cgraph * cgraph, struct ggml_cpl
     GGML_ASSERT(cplan->n_threads > 0);
     GGML_ASSERT(cplan->work_size == 0 || cplan->work_data != NULL);
 
+#if GGML_USE_KLEIDIAI
+    for (int i = 0; i < cgraph->n_nodes; i++) {
+        ggml_kai_prepare_const_data(cgraph->nodes[i]);
+    }
+#endif
+
     int n_threads                               = cplan->n_threads;
     struct ggml_threadpool * threadpool = cplan->threadpool;
 
diff --git a/ggml/src/ggml-kleidiai.cpp b/ggml/src/ggml-kleidiai.cpp
new file mode 100644
index 00000000..645f4997
--- /dev/null
+++ b/ggml/src/ggml-kleidiai.cpp
@@ -0,0 +1,930 @@
+/*
+ * Copyright (c) 2024 Arm Limited.
+ *
+ * SPDX-License-Identifier: MIT
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in all
+ * copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#if defined(__aarch64__)
+#include "ggml-kleidiai.h"
+
+#include "ggml.h"
+#include "ggml-cpu.h"
+#include "ggml-impl.h"
+#include "ggml-quants.h"
+#include "ggml-backend-impl.h"
+
+#include <arm_neon.h>
+#include <assert.h>
+#include <cfloat>
+#include <stdint.h>
+#include <string.h>
+#if defined(__linux__)
+#include <asm/hwcap.h>
+#include <sys/auxv.h>
+#elif defined(__APPLE__)
+#include <string_view>
+#include <sys/sysctl.h>
+#include <sys/types.h>
+#elif defined(_WIN32)
+#include <windows.h>
+#include <excpt.h>
+#endif
+#if defined(GGML_KLEIDIAI_USE_CACHE)
+#if !(defined(__linux__) || defined(__APPLE__))
+#error "GGML_KLEIDIAI_USE_CACHE is only supported on Linux and macOS"
+#endif
+#include <cstring>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <unistd.h>
+#endif
+
+// KleidiAI micro-kernels
+#include "kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_interface.h"
+#include "kai_lhs_quant_pack_qsi8d32p_f32.h"
+#include "kai_lhs_quant_pack_qsi8d32p_f32_neon.h"
+#include "kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0.h"
+#include "kai_rhs_pack_nxk_qsi4c32ps1s0scalef16_qsu4c32s16s0_neon.h"
+#include "kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h"
+#include "kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod.h"
+#include "kai_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod.h"
+#include "kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h"
+#include "kai_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa.h"
+#include "kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.h"
+#include "kai_common.h"
+
+#define GGML_KAI_UNUSED(x) (void)(x)
+#define MAX_EXTRA_BUFFERS 4096
+
+static const size_t k_q4_0_block_size = 32;
+
+static bool g_kai_loaded = false;
+
+// Basic backend memory allocator
+static uint8_t* g_extra_mem[MAX_EXTRA_BUFFERS];
+static int32_t g_extra_mem_idx = 0;
+
+typedef void (*kai_matmul_func_t)(const struct ggml_compute_params * params, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst);
+
+struct ggml_kai_matmul_lhs_packing_params {
+    size_t mr          = 1;
+    size_t kr          = 1;
+    size_t sr          = 1;
+    size_t packed_size = 1;
+    void (*pack_func)(size_t m, size_t k, size_t bl, size_t mr, size_t kr, size_t sr, size_t m_idx_start, const float* lhs,
+    size_t lhs_stride, void* lhs_packed) = NULL;
+};
+
+struct ggml_kai_matmul_rhs_packing_params {
+    size_t nr          = 1;
+    size_t kr          = 1;
+    size_t sr          = 1;
+    size_t packed_size = 1;
+    void (*pack_func)(size_t num_groups, size_t n, size_t k, size_t nr, size_t kr, size_t sr, size_t bl, const uint8_t* rhs, const float* bias, void* rhs_packed, size_t extra_bytes,
+                      const struct kai_rhs_pack_qs4cxs1s0_param* params) = NULL;
+};
+
+struct ggml_kai_matmul_function {
+    kai_matmul_func_t matmul = nullptr;
+};
+
+struct cpu_features {
+    bool neon{false};
+    bool dot{false};
+    bool i8mm{false};
+    bool sme{false};
+};
+
+#define KAI_FEATURE_HWCAP_ASIMD     (1 << 1)
+#define KAI_FEATURE_HWCAP_ASIMDDP   (1 << 20)
+#define KAI_FEATURE_HWCAP2_I8MM     (1 << 13)
+#define KAI_FEATURE_HWCAP2_SME      (1 << 23)
+
+#if __ANDROID_API__ >= 18
+unsigned long int getauxval(unsigned long int __type) __INTRODUCED_IN(18);
+#endif
+
+#if defined(GGML_KLEIDIAI_USE_CACHE)
+struct binary_data {
+    void *ptr;
+    size_t size;
+};
+
+struct cached_weight {
+    int fd;
+    binary_data data;
+};
+
+static const char *g_cache_filename = "kai_transformed_weights.cache";
+static const size_t g_cache_key_size = 16;
+
+static struct cached_weight g_kai_cached_weight;
+
+static void ggml_kai_open_cached_weight() {
+    if (access(g_cache_filename, F_OK) != 0) {
+        g_kai_cached_weight.fd = open(g_cache_filename, O_RDWR | O_CREAT, 0644);
+        if (g_kai_cached_weight.fd == -1) {
+            GGML_ASSERT(false);
+        }
+        g_kai_cached_weight.data.size = 0;
+    }
+    else {
+        struct stat file_info;
+        g_kai_cached_weight.fd = open(g_cache_filename, O_RDONLY);
+        if (fstat(g_kai_cached_weight.fd, &file_info) == -1) {
+            GGML_ASSERT(false);
+        }
+
+        g_kai_cached_weight.data.size = file_info.st_size;
+
+        if (g_kai_cached_weight.data.size > 0) {
+            g_kai_cached_weight.data.ptr = mmap(NULL, g_kai_cached_weight.data.size, PROT_READ, MAP_PRIVATE, g_kai_cached_weight.fd, 0);
+            if (g_kai_cached_weight.data.ptr == MAP_FAILED) {
+                GGML_ASSERT(false);
+            }
+        }
+
+    }
+}
+
+static void ggml_kai_write_cache_weight(int fd, void *key, size_t key_size, void *data, size_t data_size) {
+    if (write(fd, key, key_size) != static_cast<ssize_t>(key_size)) {
+        GGML_ASSERT(false);
+    }
+
+    if (write(fd, &data_size, sizeof(size_t)) != sizeof(size_t)) {
+        GGML_ASSERT(false);
+    }
+
+    if (write(fd, data, data_size) != static_cast<ssize_t>(data_size)) {
+        GGML_ASSERT(false);
+    }
+}
+
+static bool ggml_kai_match_cached_weight(void *token, struct binary_data *data) {
+    char* data_ptr = static_cast<char*>(g_kai_cached_weight.data.ptr);
+    char* end_ptr = data_ptr + g_kai_cached_weight.data.size;
+
+    while (data_ptr < end_ptr) {
+        void *key = data_ptr;
+        data_ptr += g_cache_key_size;
+
+        data->size=*(std::size_t*)data_ptr;
+        data_ptr += sizeof(std::size_t);
+
+        data->ptr = data_ptr;
+        data_ptr += data->size;
+
+        if (memcmp(token, key, 16) == 0) {
+            return true;
+        }
+    }
+    return false;
+}
+#endif
+
+inline bool is_feature_supported(uint64_t features, uint64_t feature_mask) {
+    return (features & feature_mask);
+}
+
+#if defined(__APPLE__)
+template <typename T>
+T get_sysctl_by_name(std::string_view name) {
+    T value{};
+    size_t size = sizeof(T);
+    if (sysctlbyname(name.data(), &value, &size, nullptr, 0) != 0) {
+        value = 0;
+    }
+    return value;
+}
+#endif
+
+#if defined(_WIN32)
+inline bool is_feature_supported(DWORD feature) {
+    return IsProcessorFeaturePresent(feature);
+}
+
+#pragma optimize("", off)  // Disable optimization for the exception handling
+bool check_i8mm_support() {
+    bool i8mm_supported = true;
+    __try {
+        int8x16_t matA = vdupq_n_s8(1);
+        int8x16_t matB = vdupq_n_s8(2);
+        int32x4_t matC = vmmlaq_s32(vdupq_n_s32(0), matA, matB);
+        int32_t array[4];
+        vst1q_s32(array, matC);
+        for (int i = 0; i < 4; ++i) {
+            assert(array[i]== 16);
+        }
+    }
+    __except (GetExceptionCode() == STATUS_ILLEGAL_INSTRUCTION ? 1 : 0)
+    {
+        i8mm_supported = false;
+    }
+    return i8mm_supported;
+}
+
+bool check_sme_support() {
+    bool sme_supported = true;
+
+    __try {
+        // Initialize two vectors for testing SME operations.
+        int8x16_t matA = vdupq_n_s8(1);
+        int8x16_t matB = vdupq_n_s8(2);
+
+        // Perform an SME-specific matrix multiplication operation (SMMLA).
+        int32x4_t matC = smmlaq_s32(vdupq_n_s32(0), matA, matB);
+
+        // Store and verify the result
+        int32_t array[4];
+        vst1q_s32(array, matC);
+        for (int i = 0; i < 4; ++i) {
+            assert(array[i] == 16);
+        }
+    }
+    __except (GetExceptionCode() == STATUS_ILLEGAL_INSTRUCTION ? 1 : 0) {
+        sme_supported = false;
+    }
+
+    return sme_supported;
+}
+#pragma optimize("", on)  // Re-enable optimization
+#endif
+
+static void get_cpu_features_impl(cpu_features &isa) {
+#if defined (__linux__)
+    const uint32_t hwcaps   = getauxval(AT_HWCAP);
+    const uint32_t hwcaps2  = getauxval(AT_HWCAP2);
+
+    isa.neon = is_feature_supported(hwcaps, KAI_FEATURE_HWCAP_ASIMD);
+    isa.dot  = is_feature_supported(hwcaps, KAI_FEATURE_HWCAP_ASIMDDP);
+    isa.i8mm = is_feature_supported(hwcaps2, KAI_FEATURE_HWCAP2_I8MM);
+    isa.sme  = is_feature_supported(hwcaps2, KAI_FEATURE_HWCAP2_SME);
+
+#elif defined(__APPLE__)
+    isa.neon = get_sysctl_by_name<uint32_t>("hw.optional.AdvSIMD") == 1;
+    isa.dot = get_sysctl_by_name<uint32_t>("hw.optional.arm.FEAT_DotProd") == 1;
+    isa.i8mm = get_sysctl_by_name<uint32_t>("hw.optional.arm.FEAT_I8MM") == 1;
+    isa.sme  = get_sysctl_by_name<uint32_t>("hw.optional.arm.FEAT_SME") == 1;
+
+#elif defined(_WIN32)
+    isa.neon = is_feature_supported(PF_ARM_NEON_INSTRUCTIONS_AVAILABLE);
+    isa.dot  = is_feature_supported(PF_ARM_V8_INSTRUCTIONS_AVAILABLE);
+    isa.i8mm = check_i8mm_support();
+    isa.sme  = check_sme_support();
+#endif
+    GGML_LOG_INFO("KleidiAI: CPU features:\n");
+    GGML_LOG_INFO("-- neon: %s\n", isa.neon? "yes" : "no");
+    GGML_LOG_INFO("-- dotprod: %s\n", isa.dot? "yes" : "no");
+    GGML_LOG_INFO("-- i8mm: %s\n", isa.i8mm? "yes" : "no");
+    GGML_LOG_INFO("-- sme: %s\n", isa.sme? "yes" : "no");
+}
+
+static const cpu_features& get_cpu_features() {
+    static cpu_features isa;
+    static bool initialized = false;
+
+    if (!initialized) {
+        get_cpu_features_impl(isa);
+        initialized = true;
+    }
+    return isa;
+}
+
+typedef void (*ggml_kai_func_t)(const struct ggml_compute_params * params, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst);
+
+bool ggml_kai_loaded(void) {
+    return g_kai_loaded;
+}
+
+void ggml_kai_init(void) {
+    static bool initialized = false;
+
+    if (!initialized) {
+        // Free previously allocated memory
+        ggml_kai_free_extra_mem();
+        initialized = true;
+        g_kai_loaded = true;
+
+#if defined(GGML_KLEIDIAI_USE_CACHE)
+        ggml_kai_open_cached_weight();
+#endif
+    }
+}
+
+bool ggml_kai_can_accelerate_matmul(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst) {
+    if(!g_kai_loaded) {
+        return false;
+    }
+    const cpu_features& cpu = get_cpu_features();
+
+    // Check whether the target platform has i8mm and dotprod and sme features
+    if(!(cpu.i8mm || cpu.dot || cpu.sme)) {
+        return false;
+    }
+
+    // Check data type support for matmul
+    // At the moment, it only works for Q4
+    if ((src1->type == GGML_TYPE_F32) && (src0->type == GGML_TYPE_Q4_0) && (dst->type == GGML_TYPE_F32)) {
+
+        // Check whether matmul is for token_embd layer. If so, the weights are required by other functions
+        // and cannot be packed
+#if defined(GGML_KLEIDIAI_REUSE_MEMORY)
+        if (!strcmp (src0->name, "token_embd.weight")) {
+            return false;
+        }
+#endif
+
+        // Check whether it batched matrix multiplication
+        if(src1->ne[2] == 1 && src1->ne[3]) {
+            const size_t n = src0->ne[1];
+            const size_t k = src1->ne[0];
+
+            // Check whether K is multiple of k_q4_0_block_size (32)
+            if(k % k_q4_0_block_size != 0) {
+                return false;
+            }
+
+            // Check whether N is multiple of 4
+            // Attention; n0 should be returned by the KleidiAI heuristic
+            const size_t n0 = 4;
+            if(n % n0 != 0) {
+                return false;
+            }
+
+            return true;
+        } else {
+            return false;
+        }
+    } else {
+        return false;
+    }
+}
+
+static kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ggml_kai_select_matmul_ukernel(size_t m, size_t n, size_t k) {
+    kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel v {};
+
+    GGML_KAI_UNUSED(n);
+    GGML_KAI_UNUSED(k);
+
+    // Get CPU features
+    const cpu_features& cpu = get_cpu_features();
+
+#if (defined(__ARM_FEATURE_SVE2) || defined(__ARM_FEATURE_SME2)) && defined(__ARM_FEATURE_MATMUL_INT8) && defined(__ARM_FEATURE_DOTPROD)
+    if (cpu.sme) {
+        if(m == 1) {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot;
+        } else {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa;
+        }
+    }
+    else if(cpu.i8mm && cpu.dot) {
+        if(m == 1) {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+        } else {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+        }
+    }
+    else if(cpu.dot) {
+        if(m == 1) {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+        } else {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+        }
+    }
+    else {
+        GGML_ASSERT(false);
+    }
+#elif defined(__ARM_FEATURE_MATMUL_INT8) && defined(__ARM_FEATURE_DOTPROD)
+    if(cpu.i8mm && cpu.dot) {
+        if(m == 1) {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod;
+        } else {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm;
+        }
+    }
+    else if(cpu.dot) {
+        if(m == 1) {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+        } else {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+        }
+    }
+    else {
+        GGML_ASSERT(false);
+    }
+#elif defined(__ARM_FEATURE_DOTPROD)
+    if(cpu.dot) {
+        if(m == 1) {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod;
+        } else {
+            v.get_m_step = kai_get_m_step_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_n_step = kai_get_n_step_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_mr = kai_get_mr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_nr = kai_get_nr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_kr = kai_get_kr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_sr = kai_get_sr_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_lhs_packed_offset = kai_get_lhs_packed_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_rhs_packed_offset = kai_get_rhs_packed_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_dst_offset = kai_get_dst_offset_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.get_dst_size = kai_get_dst_size_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+            v.run_matmul = kai_run_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod;
+        }
+    } else {
+        GGML_ASSERT(false);
+    }
+#else
+    GGML_ASSERT(false);
+#endif
+    return v;
+}
+
+static ggml_kai_matmul_lhs_packing_params ggml_kai_init_matmul_lhs_packing_params(
+    const kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel* ukernel,
+    size_t m,
+    size_t k) {
+
+    // Get CPU features
+    const cpu_features& cpu = get_cpu_features();
+
+    ggml_kai_matmul_lhs_packing_params v;
+
+    v.mr          = ukernel->get_mr();
+    v.kr          = ukernel->get_kr();
+    v.sr          = ukernel->get_sr();
+
+    if (cpu.sme) {
+        v.packed_size = kai_get_lhs_packed_size_lhs_quant_pack_qsi8d32p_f32_neon(m, k, k_q4_0_block_size /* 32 */ , v.mr, v.kr, v.sr);
+        v.pack_func = kai_run_lhs_quant_pack_qsi8d32p_f32_neon;
+    } else {
+        v.packed_size = kai_get_lhs_packed_size_lhs_quant_pack_qsi8d32p_f32(m, k, k_q4_0_block_size /* 32 */ , v.mr, v.kr, v.sr);
+        v.pack_func = kai_run_lhs_quant_pack_qsi8d32p_f32;
+    }
+
+    return v;
+}
+
+static ggml_kai_matmul_rhs_packing_params ggml_kai_init_matmul_rhs_packing_params(
+    const kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel* ukernel,
+    size_t n,
+    size_t k) {
+
+    // Get CPU features
+    const cpu_features& cpu = get_cpu_features();
+
+    ggml_kai_matmul_rhs_packing_params v;
+    v.nr          = ukernel->get_nr();
+    v.kr          = ukernel->get_kr();
+    v.sr          = ukernel->get_sr();
+
+    if (cpu.sme) {
+        v.packed_size = kai_get_rhs_packed_size_rhs_pack_nxk_qsi4c32ps1s0scalef16_qsu4c32s16s0_neon(n, k, v.nr, v.kr, k_q4_0_block_size /* 32 */);
+        v.pack_func = kai_run_rhs_pack_nxk_qsi4c32ps1s0scalef16_qsu4c32s16s0_neon;
+    } else {
+        v.packed_size = kai_get_rhs_packed_size_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0(n, k, v.nr, v.kr, k_q4_0_block_size /* 32 */);
+        v.pack_func = kai_run_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0;
+    }
+
+    return v;
+}
+
+static void ggml_kai_matmul_f32_q8c_q4c(
+    const struct ggml_compute_params * params,
+    const ggml_tensor * src0,
+    const ggml_tensor * src1,
+    ggml_tensor * dst) {
+    GGML_TENSOR_BINARY_OP_LOCALS
+
+    const int ith = params->ith;
+    const int nth = params->nth;
+
+    const enum ggml_type type = src0->type;
+
+    GGML_ASSERT(ne0 == ne01);
+    GGML_ASSERT(ne1 == ne11);
+    GGML_ASSERT(ne2 == ne12);
+    GGML_ASSERT(ne3 == ne13);
+
+    // we don't support permuted src0 or src1
+    GGML_ASSERT(nb00 == ggml_type_size(type));
+    GGML_ASSERT(nb10 == ggml_type_size(src1->type));
+
+    // dst cannot be transposed or permuted
+    GGML_ASSERT(nb0 == sizeof(float));
+    GGML_ASSERT(nb0 <= nb1);
+    GGML_ASSERT(nb1 <= nb2);
+    GGML_ASSERT(nb2 <= nb3);
+
+    const size_t m = ne11;
+    const size_t n = ne01;
+    const size_t k = ne00;
+
+    const kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel            = ggml_kai_select_matmul_ukernel(m, n, k);
+    const ggml_kai_matmul_lhs_packing_params            lhs_packing_params  = ggml_kai_init_matmul_lhs_packing_params(&ukernel, m, k);
+    const ggml_kai_matmul_rhs_packing_params            rhs_packing_params  = ggml_kai_init_matmul_rhs_packing_params(&ukernel, n, k);
+
+    GGML_ASSERT(lhs_packing_params.kr == rhs_packing_params.kr);
+    GGML_ASSERT(lhs_packing_params.sr == rhs_packing_params.sr);
+
+    // Get packing parameters
+    const size_t n_step = ukernel.get_n_step();
+
+    // Calculate number of columns to be processed per thread
+    const size_t num_n_per_thread = kai_roundup(kai_roundup(n, nth) / nth, ukernel.get_n_step());
+    const size_t n_start = ith * num_n_per_thread;
+
+    size_t n_to_process = num_n_per_thread;
+    if ((n_start + n_to_process) > n) {
+        n_to_process = n - n_start;
+    }
+
+    const uint8_t* lhs        = (const uint8_t*)src1->data;
+    uint8_t* lhs_packed       = (uint8_t*)params->wdata;
+    const uint8_t* rhs_packed = (const uint8_t*)src0->extra;
+
+    if (ith == 0) {
+        const size_t mr = lhs_packing_params.mr;
+        const size_t kr = lhs_packing_params.kr;
+        const size_t sr = lhs_packing_params.sr;
+
+        GGML_ASSERT(src1->type == GGML_TYPE_F32);
+
+        const size_t src_stride = src1->nb[1];
+
+        const size_t t_size = lhs_packing_params.packed_size;
+        GGML_ASSERT(params->wsize >= t_size);
+
+        const size_t lhs_offset = kai_get_lhs_offset_lhs_quant_pack_qsi8d32p_f32(0, src_stride);
+        const size_t lhs_packed_offset = kai_get_lhs_packed_offset_lhs_quant_pack_qsi8d32p_f32(0, k, k_q4_0_block_size /* 32 */, mr, kr, sr);
+
+        const float* src_ptr = (const float*)((const uint8_t*)lhs + lhs_offset);
+        void*        dst_ptr = (void *)((uint8_t*)lhs_packed + lhs_packed_offset);
+
+        lhs_packing_params.pack_func(
+            m, k,               // Dimensions
+            k_q4_0_block_size,  // Block length (32)
+            mr, kr, sr,         // Packing arguments
+            0,                  // M first index
+            src_ptr,            // LHS
+            src_stride,         // LHS stride
+            dst_ptr);           // LHS packed
+    }
+
+    ggml_barrier(params->threadpool);
+
+    const size_t dst_stride = dst->nb[1];
+
+    const size_t lhs_packed_offset = ukernel.get_lhs_packed_offset(0, k, k_q4_0_block_size /* 32 */);
+    const size_t rhs_packed_offset = ukernel.get_rhs_packed_offset(n_start, k, k_q4_0_block_size /* 32 */);
+    const size_t dst_offset        = ukernel.get_dst_offset(0, n_start, dst_stride);
+
+    const void* lhs_ptr = (const void*)((const char *)lhs_packed + lhs_packed_offset);
+    const void* rhs_ptr = (const void*)((const char *)rhs_packed + rhs_packed_offset);
+    float* dst_ptr = (float*)((uint8_t*)dst->data + dst_offset);
+
+    ukernel.run_matmul(
+        m, n_to_process, k,         // Dimensions
+        k_q4_0_block_size,          // Block length (32)
+        lhs_ptr,                    // LHS packed
+        rhs_ptr,                    // RHS packed
+        dst_ptr,                    // Destination
+        dst_stride,                 // Destination row stride
+        sizeof(float),              // Destination column stride
+        -FLT_MAX, FLT_MAX);         // Min and max values for the clamping operation
+}
+
+static void ggml_kai_matmul(const struct ggml_compute_params * params, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {
+
+    if((src1->type == GGML_TYPE_F32) && (dst->type == GGML_TYPE_F32)) {
+        switch (src0->type) {
+            case GGML_TYPE_Q4_0:
+                ggml_kai_matmul_f32_q8c_q4c(params, src0, src1, dst);
+                break;
+            default:
+                GGML_ASSERT(false);
+                break;
+        }
+    } else {
+        GGML_ASSERT(false);
+    }
+}
+
+static void ggml_kai_matmul_rhs_pack(ggml_tensor * cur) {
+    const size_t n = cur->ne[1];
+    const size_t k = cur->ne[0];
+
+    const kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel = ggml_kai_select_matmul_ukernel(1, n, k);
+    const ggml_kai_matmul_rhs_packing_params rhs_packing_params = ggml_kai_init_matmul_rhs_packing_params(&ukernel, n, k);
+
+    if (cur->extra == NULL) {
+        if(cur->type == GGML_TYPE_Q4_0) {
+#if defined(GGML_KLEIDIAI_USE_CACHE)
+            if (g_kai_cached_weight.data.size > 0) {
+                struct binary_data data;
+                bool matched = ggml_kai_match_cached_weight(cur->data, &data);
+                if (matched) {
+                    cur->extra = data.ptr;
+                }
+                else {
+                    perror("No match found, please remove the cache file and try again!");
+                    GGML_ASSERT(false);
+                }
+                return;
+            }
+#endif
+            const size_t original_data_size = ggml_nbytes(cur);
+            const size_t reshaped_data_sz = rhs_packing_params.packed_size;
+
+            // Temporary memory for the computation.
+            uint8_t *reshaped_data = (uint8_t*)malloc(reshaped_data_sz);
+
+            struct kai_rhs_pack_qs4cxs1s0_param params;
+            params.lhs_zero_point = 1;
+            params.rhs_zero_point = 8;
+
+            rhs_packing_params.pack_func(
+                1, n, k,                    // Dimensions
+                rhs_packing_params.nr,      // Nr
+                rhs_packing_params.kr,      // Kr
+                rhs_packing_params.sr,      // Sr
+                k_q4_0_block_size,          // Block length (32)
+                (const uint8_t*)cur->data,  // RHS
+                NULL,                       // Bias
+                reshaped_data,              // RHS PACKED
+                0,
+                &params);
+#if defined(GGML_KLEIDIAI_USE_CACHE)
+            ggml_kai_write_cache_weight(g_kai_cached_weight.fd, cur->data, g_cache_key_size, reshaped_data, reshaped_data_sz);
+#endif
+
+#if defined(GGML_KLEIDIAI_REUSE_MEMORY)
+            GGML_ASSERT(reshaped_data_sz <= original_data_size);
+            memcpy(cur->data, (void *)reshaped_data, original_data_size);
+            free(reshaped_data);
+            cur->extra = cur->data;
+#else
+            g_extra_mem[g_extra_mem_idx++] = reshaped_data;
+            cur->extra = reshaped_data;
+#endif
+        } else {
+            GGML_ASSERT(false);
+        }
+    }
+}
+
+bool ggml_kai_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {
+    if (!g_kai_loaded) return false;
+
+    // tensor refers to the destination tensor and has the "src" member to get the pointers
+    // to the source tensors required to perform the operation
+    // tensor         = destination
+    // tensor->src[0] = first source tensor
+    // tensor->src[1] = second source tensor
+
+    ggml_kai_func_t func;
+    const bool is_cpu_only = ggml_backend_buffer_is_host(tensor->buffer)
+        || (tensor->src[0] != nullptr && ggml_backend_buffer_is_host(tensor->src[0]->buffer))
+        || (tensor->src[1] != nullptr && ggml_backend_buffer_is_host(tensor->src[0]->buffer));
+
+    if (!is_cpu_only) {
+        return false;
+    }
+
+    switch (tensor->op) {
+        case GGML_OP_MUL_MAT:
+            if (!ggml_kai_can_accelerate_matmul(tensor->src[0], tensor->src[1], tensor)) {
+                return false;
+            }
+
+            func = ggml_kai_matmul;
+            break;
+        default:
+            return false;
+    }
+
+    func(params, tensor->src[0], tensor->src[1], tensor);
+
+    return true;
+}
+
+bool ggml_kai_prepare_const_data(struct ggml_tensor * tensor) {
+    if (!g_kai_loaded) return false;
+
+    // tensor refers to the destination tensor and has the "src" member to get the pointers
+    // to the source tensors required to perform the operation
+    // tensor         = destination
+    // tensor->src[0] = first source tensor
+    // tensor->src[1] = second source tensor
+
+    const bool is_cpu_only = ggml_backend_buffer_is_host(tensor->buffer)
+        || (tensor->src[0] != nullptr && ggml_backend_buffer_is_host(tensor->src[0]->buffer))
+        || (tensor->src[1] != nullptr && ggml_backend_buffer_is_host(tensor->src[0]->buffer));
+
+    if (!is_cpu_only) {
+        return false;
+    }
+
+    switch (tensor->op) {
+        case GGML_OP_MUL_MAT:
+            if (!ggml_kai_can_accelerate_matmul(tensor->src[0], tensor->src[1], tensor)) {
+                return false;
+            }
+            ggml_kai_matmul_rhs_pack(tensor->src[0]);
+            break;
+        default:
+            return false;
+    }
+
+    return true;
+}
+
+size_t ggml_kai_get_temp_workspace_size_matmul(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst) {
+
+    const size_t m = src1->ne[1];
+    const size_t k = src1->ne[0];
+
+    if (ggml_kai_can_accelerate_matmul(src0, src1, dst)) {
+        const kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel = ggml_kai_select_matmul_ukernel(m, 1, k);
+        const ggml_kai_matmul_lhs_packing_params lhs_packing_params = ggml_kai_init_matmul_lhs_packing_params(&ukernel, m, k);
+        return lhs_packing_params.packed_size;
+    }
+    return 0;
+}
+
+size_t ggml_kai_get_const_workspace_size_matmul(const struct ggml_tensor * cur) {
+
+    if(cur->type == GGML_TYPE_Q4_0) {
+        const int32_t n = cur->ne[1];
+        const int32_t k = cur->ne[0];
+        const int32_t b = cur->ne[2];
+
+        // Temporary solution as we should check whether we can run the kleidiai matmul micro-kernels
+        const cpu_features& cpu = get_cpu_features();
+
+        // Check whether the target platform has i8mm and dotprod and sme features
+        if(!(cpu.i8mm || cpu.dot || cpu.sme)) {
+            return false;
+        }
+
+        const kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_ukernel ukernel = ggml_kai_select_matmul_ukernel(1, n, k);
+        const ggml_kai_matmul_rhs_packing_params rhs_packing_params = ggml_kai_init_matmul_rhs_packing_params(&ukernel, n, k);
+
+        // Attention: It is not sufficient to check the weights name to know whether or not
+        // we should run the optimized micro-kernel. This approach is temporary.
+        // The right approach should check whether the weights are used in the other layers.
+        // If the weights are only only used for the matmul operator, we should use the optimized micro-kernel.
+        #if defined(GGML_KLEIDIAI_REUSE_MEMORY)
+        if (!strcmp (cur->name, "token_embd.weight")) {
+            return 0;
+        }
+        #endif
+        if ((b == 1)) {
+            return rhs_packing_params.packed_size;
+        }
+    }
+
+    return 0;
+}
+
+void ggml_kai_free_extra_mem(void) {
+    for(int32_t i = g_extra_mem_idx - 1; i >= 0; i--) {
+        free(g_extra_mem[i]);
+    }
+    g_extra_mem_idx = 0;
+
+#if defined(GGML_KLEIDIAI_USE_CACHE)
+    if (g_kai_cached_weight.data.size > 0) {
+        munmap(g_kai_cached_weight.data.ptr, g_kai_cached_weight.data.size);
+    }
+    close(g_kai_cached_weight.fd);
+#endif
+}
+#endif // defined(__aarch64__)
diff --git a/ggml/src/ggml-kleidiai.h b/ggml/src/ggml-kleidiai.h
new file mode 100644
index 00000000..26d2af8f
--- /dev/null
+++ b/ggml/src/ggml-kleidiai.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2024 Arm Limited.
+ *
+ * SPDX-License-Identifier: MIT
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in all
+ * copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#pragma once
+
+#include "ggml.h"
+#include "ggml-backend.h"
+
+#ifdef  __cplusplus
+extern "C" {
+#endif
+
+bool ggml_kai_loaded(void);
+void ggml_kai_init(void);
+bool ggml_kai_can_accelerate_matmul(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst);
+bool ggml_kai_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor);
+bool ggml_kai_prepare_const_data(struct ggml_tensor * tensor);
+void ggml_kai_free_extra_mem(void);
+size_t ggml_kai_get_temp_workspace_size_matmul(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst);
+size_t ggml_kai_get_const_workspace_size_matmul(const struct ggml_tensor * cur);
+
+#ifdef  __cplusplus
+}
+#endif
diff --git a/ggml/src/ggml.c b/ggml/src/ggml.c
index e6a7824b..43ed99e0 100644
--- a/ggml/src/ggml.c
+++ b/ggml/src/ggml.c
@@ -31,6 +31,10 @@
 #include <syscall.h>
 #endif
 
+#ifdef GGML_USE_KLEIDIAI
+#include "ggml-kleidiai.h"
+#endif
+
 #if defined(__APPLE__)
 #include <unistd.h>
 #include <mach/mach.h>
@@ -1407,6 +1411,11 @@ static inline bool ggml_can_repeat_rows(const struct ggml_tensor * t0, const str
 ////////////////////////////////////////////////////////////////////////////////
 
 struct ggml_context * ggml_init(struct ggml_init_params params) {
+
+#if defined(GGML_USE_KLEIDIAI)
+        ggml_kai_init();
+#endif
+
     static bool is_first_call = false;
 
     ggml_critical_section_start();
@@ -6794,6 +6803,10 @@ void ggml_quantize_free(void) {
     iq2xs_free_impl(GGML_TYPE_IQ1_S);
     iq3xs_free_impl(256);
 
+#if GGML_USE_KLEIDIAI
+    ggml_kai_free_extra_mem();
+#endif
+
     ggml_critical_section_end();
 }
 
diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index 46a6ad56..9e92fb49 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -22,6 +22,10 @@ add_library(llama
             unicode-data.cpp
             )
 
+if (GGML_KLEIDIAI)
+add_compile_definitions(GGML_USE_KLEIDIAI)
+endif()
+
 target_include_directories(llama PUBLIC . ../include)
 target_compile_features   (llama PUBLIC cxx_std_11) # don't bump
 
diff --git a/src/llama.cpp b/src/llama.cpp
index 0cdf0c07..20397e40 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -1903,8 +1903,19 @@ struct llama_mmap {
 
     llama_mmap(const llama_mmap &) = delete;
 
-#ifdef _POSIX_MAPPED_FILES
+#if !defined(GGML_USE_KLEIDIAI) && (defined(_POSIX_MAPPED_FILES) || defined(_WIN32))
+    // With KleidiAI, we disable mmap to allow the backend
+    // to re-use the memory allocated for the weights.
+    // KleidiAI requires to pack the weights in a different format from the original one
+    // to improve the overall computational efficiency.
+    // However, since RAM is very limited on some devices, we want to re-use the original
+    // storage to avoid allocating additional memory.
     static constexpr bool SUPPORTED = true;
+#else
+    static constexpr bool SUPPORTED = false;
+#endif
+
+#ifdef _POSIX_MAPPED_FILES
 
     // list of mapped fragments (first_offset, last_offset)
     std::vector<std::pair<size_t, size_t>> mapped_fragments;
@@ -2016,7 +2027,6 @@ struct llama_mmap {
         }
     }
 #elif defined(_WIN32)
-    static constexpr bool SUPPORTED = true;
 
     llama_mmap(struct llama_file * file, size_t prefetch = (size_t) -1, bool numa = false) {
         GGML_UNUSED(numa);
@@ -2078,7 +2088,6 @@ struct llama_mmap {
         }
     }
 #else
-    static constexpr bool SUPPORTED = false;
 
     llama_mmap(struct llama_file * file, size_t prefetch = -1, bool numa = false) {
         GGML_UNUSED(file);
-- 
2.39.5 (Apple Git-154)

