{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ede63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Copyright (c) 2021 Arm Limited. All rights reserved.\n",
    "#  SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f978ca1",
   "metadata": {},
   "source": [
    "#  Fast Inference on Arm® Ethos™-U55 microNPU with Arm ML Embedded Evaluation Kit\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2bcfe",
   "metadata": {},
   "source": [
    "In this Notebook you will see how to build and run a micro speech command model targeting Arm® Cortex™-M55 CPU and\n",
    "Arm® Ethos™-U55 using Arm ML Embedded Evaluation Kit and Arm Virtual Hardware.\n",
    "The [Ethos-U55 microNPU](https://www.arm.com/products/silicon-ip-cpu/ethos/ethos-u55) is a first generation microNPU designed\n",
    "to accelerate computation for ML workloads in resource-constrained embedded and IoT devices. Its advanced compression\n",
    "techniques save power, and reduce ML model sizes significantly to enable execution of neural networks that previously\n",
    "only ran on larger systems. Ethos-U microNPU works with Cortex-M CPU devices and Arm Corstone™ systems and allows developers\n",
    "to configure and build high performance, power efficient SoCs while differentiating with combinations of Arm processors\n",
    "and their own IP.\n",
    " \n",
    " \n",
    "The **Arm ML Embedded Evaluation Kit** allows developers to quickly build and deploy embedded machine learning\n",
    "applications for Arm Cortex-M55 CPU and Arm Ethos-U55 microNPU. With ML Embedded Evaluation Kit you can run inferences by\n",
    "using either a custom neural network on Ethos-U microNPU or pre-built ML applications such as image classification,\n",
    "keyword spotting (KWS), automated speech recognition (ASR), anomaly detection, and person detection all using Arm Fixed\n",
    "Virtual Platform (FVP) available in Arm Virtual Hardware.\n",
    "\n",
    "The **Arm Virtual Hardware** is an accurate representation of a physical SoC and it runs as a simple application in a\n",
    "Linux environment for easy scalability in the cloud and removes dependency on silicon availability.\n",
    "Powered by Amazon Web Services (AWS), developers can launch Amazon Machine Image (AMI) running as a virtual server in\n",
    "the cloud called Arm Virtual Hardware which is configured with Arm development tools for IoT, Machine learning, and\n",
    "embedded applications, Arm Compilers, Fixed Virtual Platforms, and other development tools targeting Cortex-M CPU and\n",
    "Ethos-U microNPU.\n",
    "\n",
    "This notebook contains the following sections:\n",
    "\n",
    "- Pre-processing the input data\n",
    "- Training the Convolutional Neural Network (CNN) model using TensorFlow\n",
    "- Optimizing and Quantizing the trained network model using TensorFlow Optimization Toolkit in order to target Ethos-U55 microNPU\n",
    "- Compiling the quantized model with Arm Vela compiler\n",
    "- Configuring and compiling the build project targeting Ethos-U55 using Arm ML Embedded Evolution Kit\n",
    "- Executing the model on Ethos-U55 microNPU using Arm Virtual Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3892362",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "#### 1. Install dependencies \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow\n",
    "!pip install matplotlib numpy IPython\n",
    "!pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d18be3",
   "metadata": {},
   "source": [
    "#### 2. Clone Arm ML Embedded Evaluation Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone \"https://review.mlplatform.org/ml/ethos-u/ml-embedded-evaluation-kit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d30a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ml-embedded-evaluation-kit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16756b6",
   "metadata": {},
   "source": [
    "#### 3. Pull all the external dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git submodule update --init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9545078",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/projects/MicroSpeechEthosU55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c5537",
   "metadata": {},
   "source": [
    "#### 4. Import necessary modules and run the code sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94ba4e",
   "metadata": {},
   "source": [
    "## Import the Speech Commands dataset\n",
    "\n",
    "The Google [speech command dataset](https://www.tensorflow.org/datasets/catalog/speech_commands)  consists of over 105,\n",
    "000 WAV audio files of people saying 30 different words which was collected by Google and released under a CC BY license.\n",
    "In this tutorial we download and extract a portion of the Speech Commands dataset containing one second WAV file of 8\n",
    "different words with 16kHz sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ec5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('data/mini_speech_commands')\n",
    "if not data_dir.exists():\n",
    "    tf.keras.utils.get_file(\n",
    "      'mini_speech_commands.zip',\n",
    "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "print('Commands:', commands)\n",
    "\n",
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples per label:',\n",
    "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d96ea",
   "metadata": {},
   "source": [
    "Splitting the dataset into three sets of training, validation and test sets to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = filenames[:6400]\n",
    "val_files = filenames[6400: 6400 + 800]\n",
    "test_files = filenames[-800:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34187822",
   "metadata": {},
   "source": [
    "## Reading audio data and their labels from audio file\n",
    "\n",
    "Each .wav file contains a header and the raw data in time format with a sampling rate of 16kHz which means that one\n",
    "second of audio has 16,000 samples.\n",
    "\n",
    "TensorFlow provides tf.io module to read audio file as a binary file and tf.audio module to process the audio. The\n",
    "tf.audio.decode_wav API decodes a given 16bit PCM wav file and returns the sample rate and the scaled decoded 16bit PCM\n",
    "wav file to the range [-1 ,1] as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465439e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "    audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "    return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab4050",
   "metadata": {},
   "source": [
    "The supervised learning algorithms require a set of inputs and corresponding outputs to learn from the data in order to\n",
    "build a predictive model. In our dataset, the label of each wav file is in its parent directory and we can get them by\n",
    "get_label method. Then, in order to assign each wav file to its corresponding label, get_waveform_and_label method is\n",
    "applied which takes the name of the wav file and outputs a tuple including the audio and associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    \n",
    "    return parts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c67f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform_and_label(file_path):\n",
    "    label = get_label(file_path)\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    waveform = decode_audio(audio_binary)\n",
    "    \n",
    "    return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f7570",
   "metadata": {},
   "source": [
    "Pre-Processing with TF.Data\n",
    "tf.data API helps you to build complex input data pipelines as well as handling large amounts of data, reading from\n",
    "different data formats such as CSV, Numpy, text, etc and perform complex transformation.\n",
    "\n",
    "Although GPUs and TPUs can significantly reduce the training time, as a deep learning developer you may experience\n",
    "not using the full capacity of your GPU with the bottleneck being on the CPU. Therefore, it is very important to ensure that we\n",
    "achieve optimal performance and efficiency in our input pipeline and with tf.data API we can address this issue.\n",
    "There are several techniques which reduce computational overhead and you can easily implement them into your pipeline\n",
    "such as:\n",
    "\n",
    "- Prefetching\n",
    "- Parallelising data extraction and transformation\n",
    "- Caching\n",
    "- Vector mapping\n",
    "\n",
    "You can read more about each technique and how they work from Google TensorFlow tutorials. Here we use parallelising\n",
    "data transformation to do some preprocessing on our dataset before passing it to the model for training.\n",
    "The tf.data.Dataset.map does the transformation (extract the audio-label pairs) and uses multiple available CPU cores for\n",
    "working during tf.data runtime with the tf.data.AUTOTUNE parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc2f6f",
   "metadata": {},
   "source": [
    "## Spectral feature extraction for audio analysis\n",
    "\n",
    "A spectrogram is a common technique to analyze audio files and sound waves. It extracts information from the signal by\n",
    "converting the waveform into a spectrogram which shows frequency changes over time and can be represented as a 2D image\n",
    "with time in x-axis and frequency in the y-axis and density of colors representing the signal strength. Hence, the\n",
    "spectrogram image explains how the strength of the signal is distributed over different frequencies.\n",
    "\n",
    "A short-time Fourier transform (tf.signal.stft.) is a technique that converts a signal to time-frequency domain and it\n",
    "generates an array of complex numbers representing magnitude (tf.abs) and phase.\n",
    "\n",
    "To obtain waveforms of the same length we can zero pad audio that is shorter than one second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    # Padding for files with less than 16000 samples\n",
    "    zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
    "    \n",
    "    # Concatenate audio with padding so that all audio clips will be of the \n",
    "    # same length\n",
    "    waveform = tf.cast(waveform, tf.float32)\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "      \n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    \n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34219c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(100):\n",
    "    label = label.numpy().decode('utf-8')\n",
    "    spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4d9d5",
   "metadata": {},
   "source": [
    "##  Visualize the waveform and spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "    log_spec = np.log(spectrogram.T)\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, log_spec, shading='auto')\n",
    "\n",
    "# training data\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7f2a9",
   "metadata": {},
   "source": [
    "Now we transform the waveform dataset to get the spectrogram images and their corresponding labels as integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d48c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_and_label_id(audio, label):\n",
    "    spectrogram = get_spectrogram(audio)\n",
    "    spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "    spectrogram =tf.image.resize(spectrogram, (32,32))\n",
    "    label_id = tf.argmax(label == commands)\n",
    "    return spectrogram, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(\n",
    "    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f471d3",
   "metadata": {},
   "source": [
    "Pre-processing for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c59034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(files):\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
    "    output_ds = output_ds.map(get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n",
    "    return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ee6c0",
   "metadata": {},
   "source": [
    "Batch the training and validation sets for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b54131",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9a9ab",
   "metadata": {},
   "source": [
    "Add dataset cache() and prefetch() operations to reduce read latency while training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0e3a1",
   "metadata": {},
   "source": [
    "## Training a CNN model\n",
    "\n",
    "Convolutional Neural Networks, or CNNs for short, are a class of deep neural networks that are designed to recognize an\n",
    "image by transforming the image via layers to class scores. Since CNNs are powerful for processing and classifying\n",
    "images and we converted the audio files into spectrogram images, a CNN model is trained here. The model contains a\n",
    "**normalization layer** to normalize each pixel in the image based on its mean and standard deviation ready for the following layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "    input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(commands)\n",
    "\n",
    "norm_layer = preprocessing.Normalization()\n",
    "norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape), \n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec92a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,  \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a356027",
   "metadata": {},
   "source": [
    "### Evaluate the model perfomance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cc935",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "    test_audio.append(audio.numpy())\n",
    "    test_labels.append(label.numpy())      \n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a262253",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197b50b",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "You can save an entire model to a single artifact including the model's architecture, weights, compilation information\n",
    "and training configuration such as optimizer, losses and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b56a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "def round_up(n, decimals=2):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "\n",
    "home_dir = Path.home()\n",
    "file_path = str(home_dir)+'/'+ 'projects'+'/'+'MicroSpeechEthosU55'\n",
    "size = (os.path.getsize(file_path+'/model.h5')/1000000)\n",
    "\n",
    "print('The size of model: {} Mb'.format(round_up(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1aae99",
   "metadata": {},
   "source": [
    "### Evaluate the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_audio, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac20739",
   "metadata": {},
   "source": [
    "## Model Optimization for inference on Ethos-U microNPU\n",
    "\n",
    "To run and accelerate an inference on edge devices, several model optimization methods can be applied to optimize\n",
    "machine learning models. TensorFlow Model Optimization Toolkit provides optimization techniques such as\n",
    "[quantization](https://www.tensorflow.org/model_optimization/guide/quantization/post_training),\n",
    "[pruning](https://www.tensorflow.org/model_optimization/guide/pruning) and\n",
    "[clustering](https://www.tensorflow.org/model_optimization/guide/clustering) compatible with TensorFlow Lite. Based on\n",
    "the optimization technique, the complexity and the size of the model can be reduced which results in less memory usage,\n",
    "smaller storage size, and download size.\n",
    "Also, optimization is required for some hardware accelerators such as Arm Ethos-U microNPU as it performs calculations in\n",
    "8-bit integer precision.\n",
    "\n",
    "### TensorFlow Model Optimization Toolkit - Weight Clustering API\n",
    "\n",
    "Weight clustering which was proposed and contributed by Arm ML Tooling team to TensorFlow Model Optimization Toolkit reduces the\n",
    "storage and the size of the model leading to benefits for deployment on resource-constrain embedded systems. With this\n",
    "technique, the size of the model will be reduced by replacing similar weights in each layer with the same value. These\n",
    "values are found by running a clustering algorithm over the weights of the trained model.\n",
    "Depending on the model and number of chosen clusters, the accuracy of the model could drop after clustering. To reduce\n",
    "the impact on accuracy, you must pass a pre-trained model with acceptable accuracy before clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c77a8b",
   "metadata": {},
   "source": [
    "#### Define the model and apply weight clustering to a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "\n",
    "clustering_params = {\n",
    "  'number_of_clusters': 32,\n",
    "  'cluster_centroids_init': CentroidInitialization.LINEAR\n",
    "}\n",
    "\n",
    "# Cluster a whole model\n",
    "clustered_model = cluster_weights(model, **clustering_params)\n",
    "\n",
    "# Use smaller learning rate for fine-tuning clustered model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "\n",
    "clustered_model.compile(\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "clustered_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa62aba",
   "metadata": {},
   "source": [
    "#### Fine Tune the model with 1 epoch and evaluate the accuracy against the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune model\n",
    "clustered_model.fit(\n",
    "  train_ds,\n",
    "    validation_data=val_ds,  \n",
    "  batch_size=500,\n",
    "  epochs=1,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9675c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clustered_model_accuracy = clustered_model.evaluate(\n",
    "  test_audio, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Clustered test accuracy:', clustered_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd366fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a compressible model for TensorFlow.\n",
    "final_model = tfmot.clustering.keras.strip_clustering(clustered_model)\n",
    "clustered_tflite_file = 'clustered_model.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "tflite_clustered_model = converter.convert()\n",
    "\n",
    "with open(clustered_tflite_file, 'wb') as f:\n",
    "    f.write(tflite_clustered_model)\n",
    "print('Saved clustered TFLite model to:', clustered_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (os.path.getsize(file_path+'/clustered_model.tflite')/1000000)\n",
    "\n",
    "print('The size of clustered model: {} Mb'.format(round_up(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82a468",
   "metadata": {},
   "source": [
    "### Create a TFLite model from combining weight clustering and post-training quantization\n",
    "\n",
    "As Ethos-U55 only supports 8-bit operations and 8 or 16 bit activations, post-training integer quantization should be applied to\n",
    "the trained TensorFlow model to convert the weights and biases from floating point numbers to integer numbers.\n",
    "Quantazation is not only supported by all CPU platforms, but also supports deploying the optimized model for special purpose\n",
    "hardware accelerators such as NPUs.\n",
    "Weight clustering can combine with quantization to improve memory footprint from both techniques and speed up inference.\n",
    "Quantization then allows the clustered model to be used with Arm Ethos-N and Ethos-U machine learning processors.\n",
    "\n",
    "Post-training integer quantization not only increases inferencing speed on microcontrollers but also is compatible with fixed-point hardware accelerators such as Arm Ethos-U and Ethos-N NPUs. It converts models’ parameters from 32-bit floating point to nearest 8-bit fixed-point numbers while getting reasonable quantized model accuracy with 3-4x reduction in model size.  \n",
    "\n",
    "\n",
    "There are two modes of post-training integer quantization: \n",
    "\n",
    "\n",
    "- Post-training integer quantization with int8 activation and weights \n",
    "\n",
    "- Post-training integer quantization with int16 activation and int8 weights (16x8 quantization mode) \n",
    "\n",
    "Quantizing using integer-only converts weights, variables, input, and output tensors to integer. TensorFlow Lite supports quantization with int16 activations and int8 weights during model conversion from TensorFlow to TensorFlow Lite’s flat buffer format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed858a0",
   "metadata": {},
   "source": [
    "With post training quantization, the weights of the model are quantized to 8bit integer values following by quantizing\n",
    "the variable tensors such as layer activations. To calculate the potential range of values that all these tensors can\n",
    "take, we need a small subset of data as a representative of model input during deployment and these samples can be taken\n",
    "from training or validation set. Model inference is then performed using this representative dataset with calculating\n",
    "minimum and maximum values for variable tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5552d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "      data = next(iter(val_ds))[0]\n",
    "      yield [data.numpy().astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "quantized_and_clustered_tflite_file = 'quantized_clustered.tflite'\n",
    "\n",
    "with open(quantized_and_clustered_tflite_file, 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print('Saved quantized and clustered TFLite model to:', quantized_and_clustered_tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76a083",
   "metadata": {},
   "source": [
    "#### See the persistence of accuracy from TF to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate the TFLite model on the test dataset\n",
    "def eval_model(interpreter,test_audio):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    \n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "\n",
    "    for i, test_audio in enumerate(test_audio):\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "        \n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_audio = np.expand_dims(test_audio, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_audio)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "        \n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9abf15",
   "metadata": {},
   "source": [
    "load the TFLite model from the disk using TensorFlow Lite Interpreter Python API for deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf4f68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Load the TFLite model in TFLite Interpreter \n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_audio = test_audio\n",
    "\n",
    "test_accuracy = eval_model( interpreter,test_audio)\n",
    "\n",
    "print('Clustered and quantized TFLite test_accuracy:', test_accuracy)\n",
    "print('Clustered TF test accuracy:', clustered_model_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (os.path.getsize(file_path+'/quantized_clustered.tflite')/1000000)\n",
    "\n",
    "print('The size of clustered and quantized TFlite model: {} Mb'.format(round_up(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd451d36",
   "metadata": {},
   "source": [
    "You can check the result of quantized TensorFlow Lite file with [Netron](https://netron.app/) and see the result of\n",
    "post-training quantization which converts the weights and activations from floating point numbers to integer numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeb6c2",
   "metadata": {},
   "source": [
    "## Compile the model for Ethos-U55 with Vela Compiler\n",
    " \n",
    "To deploy your NN model on Ethos-U55, you need to compile the trained quantized model via Vela to generate an optimized\n",
    "NN model for Ethos-U. Vela is an open source python tool which compiles a TFLite NN model into an optimized version that\n",
    "can run on an embedded system containing Arm Ethos-U microNPU.\n",
    "\n",
    "The optimized model has TensorFlow Lite custom operators (supported operators) for those parts of the model that can be\n",
    "accelerated by the Ethos-U microNPU. Parts of the model that cannot be accelerated are left unchanged and will instead run on\n",
    "the Cortex-M series CPU using an appropriate kernel.\n",
    "\n",
    "You can install Vela by running `$ pip install ethos-u-vela` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73341b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ethos-u-vela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3fa27",
   "metadata": {},
   "source": [
    "The Vela compiler accepts a set of parameters to influence model optimization. The model provided within this project\n",
    "has been optimized with the following configuration:\n",
    " \n",
    "- `accelerator-config`: specifies the NPU configuration to use between \n",
    "    - ethos-u55-256\n",
    "    - **ethos-u55-128**\n",
    "    - ethos-u55-64\n",
    "    - ethos-u55-32\n",
    "    - ethos-u65-256\n",
    "    - ethos-u65-512\n",
    "    \n",
    "- `optimise`: sets the optimization strategy to maximize the **performance** of model or minimize the memory usage.\n",
    "\n",
    "We will create a vela.ini file with our system configuration description. This information helps vela to optimize the model\n",
    "efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6838c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile vela.ini\n",
    "\n",
    "[System_Config.Ethos_U55_High_End_Embedded]\n",
    "core_clock=500e6\n",
    "axi0_port=Sram\n",
    "axi1_port=OffChipFlash\n",
    "Sram_clock_scale=1.0\n",
    "Sram_burst_length=32\n",
    "Sram_read_latency=32\n",
    "Sram_write_latency=32\n",
    "OffChipFlash_clock_scale=0.125\n",
    "OffChipFlash_burst_length=128\n",
    "OffChipFlash_read_latency=64\n",
    "OffChipFlash_write_latency=64\n",
    "\n",
    "; Shared SRAM: the SRAM is shared between the Ethos-U and the Cortex-M software\n",
    "; The non-SRAM memory is assumed to be read-only\n",
    "[Memory_Mode.Shared_Sram]\n",
    "const_mem_area=Axi1\n",
    "arena_mem_area=Axi0\n",
    "cache_mem_area=Axi0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d9b12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compile the network for an Ethos-U55 128 microNPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03339cc4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "vela --accelerator-config=ethos-u55-128 \\\n",
    "--optimise Performance \\\n",
    "--memory-mode=Shared_Sram \\\n",
    "--system-config=Ethos_U55_High_End_Embedded \\\n",
    "--config vela.ini \\\n",
    "quantized_clustered.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634be21d",
   "metadata": {},
   "source": [
    "To summarize, a neural network can be efficiently accelerated in an extremely small area and power envelope using the\n",
    "following:\n",
    "\n",
    "- TensorFlow Lite micro\n",
    "- TensorFlow Model Optimzation Toolkit\n",
    "- Ethos-U55 and Vela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0058fe",
   "metadata": {},
   "source": [
    "Finally, after the model has been compiled through Vela, the output of the tool is an optimized TensorFlow Lite file\n",
    "which is ready to deploy on a system using an Ethos-U microNPU in this case Arm Virtual Hardware configured with the\n",
    "Corstone-300 FVP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1b940",
   "metadata": {},
   "source": [
    "## ARM ML Embedded Evaluation Kit\n",
    "[ML Eval Kit](https://review.mlplatform.org/plugins/gitiles/ml/ethos-u/ml-embedded-evaluation-kit) is an open source project available under Apache 2.0 license.\n",
    "\n",
    "\n",
    "Three main functionality: \n",
    "\n",
    "- performance evaluation\n",
    "    - number of NPU cycles that are necessary to compute inference\n",
    "    - amount of memory transactions that occurred\n",
    "    \n",
    "    \n",
    "- Software stack evaluation\n",
    "    - contains developed ML applications for Ethos-U55 systems\n",
    "    - configure the build system for a default build using build_default.py \n",
    "    \n",
    "    \n",
    "- Custom workflow\n",
    "    - test custom NN performance on the Ethos-u55 with Generic Inference Runner capability\n",
    "    - configure the build system for non-default build:\n",
    "        - specify Vela configuration and compile the model,\n",
    "        - configure the build system with CMake\n",
    "        - compile the project with make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e4b14",
   "metadata": {},
   "source": [
    "### Configure the build system with CMake\n",
    "\n",
    "Configure the build project by creating a build directory in the root of the project, navigate inside and execute cmake\n",
    "with setting the locations of the TFLite file generated by Vela and the labels text file of the associated labels file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/projects/MicroSpeechEthosU55/ml-embedded-evaluation-kit/build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/projects/MicroSpeechEthosU55/ml-embedded-evaluation-kit/build/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd448b",
   "metadata": {},
   "source": [
    "\n",
    "We will use the following build options:\n",
    "\n",
    "- TARGET_PLATFORM\n",
    "- CMAKE_TOOLCHAIN_FILE\n",
    "- USE_CASE_BUILD\n",
    "- <use_case\\>\\_MODEL_TFLITE_PATH\n",
    "\n",
    "See [reference manual](https://review.mlplatform.org/plugins/gitiles/ml/ethos-u/ml-embedded-evaluation-kit/+/refs/heads/main/docs/sections/building.md#build-options)\n",
    "for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb22b2",
   "metadata": {},
   "source": [
    "Use Generic Inference Runner ML Eval Kit build option to profile inference speeds for your specific ML applications on\n",
    "Cortex-M55 CPU and Ethos-U55 microNPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cmake -DTARGET_PLATFORM=mps3 \\\n",
    "    -DCMAKE_TOOLCHAIN_FILE=../scripts/cmake/toolchains/bare-metal-gcc.cmake \\\n",
    "    -Dinference_runner_MODEL_TFLITE_PATH=/home/ubuntu/projects/MicroSpeechEthosU55/output/quantized_clustered_vela.tflite \\\n",
    "    -DUSE_CASE_BUILD=inference_runner .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1d5f7",
   "metadata": {},
   "source": [
    "### Compile the project with make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9e79e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d1d60f",
   "metadata": {},
   "source": [
    "Results of the build are placed under build/bin folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237a2a2",
   "metadata": {},
   "source": [
    "## Running the application binary on an FVP emulating MPS3 using Arm Virtual Hardware\n",
    "\n",
    "Arm Virtual Hardware provides an Ubuntu Linux image including Arm development tools for IoT, Machine Learning, and\n",
    "embedded applications. Arm Compilers, Fixed Virtual Platforms, and other development tools targeting Cortex-M CPU are\n",
    "available to get started quickly. The Arm Virtual Hardware Beta (Initial) Release is provided free of charge and may be\n",
    "used only for evaluation, for example, to evaluate development processes in CI/CD, MLOps and DevOps workflows which\n",
    "require automated testing and scalability beyond a farm of development boards.\n",
    "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instances-and-amis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52bc38",
   "metadata": {},
   "source": [
    "#### Launch the desired application on the Fixed Virtual Platform (FVP) - Corstone-300 MPS3 based platform\n",
    "\n",
    "Finally, to deploy the micro speech application on an FVP emulating MPS3 FPGA board that contains Cortex-M55 and\n",
    "Ethos-U55 processors, launch the FVP with the choice of the Ethos-U55 `$ FVP_Corstone_SSE-300_Ethos-U55`.\n",
    "\n",
    "The number of MACs on the Arm Virtual Hardware FVP execution should be the same as in the Vela compiler\n",
    "`--accelerator-config` configuration. To pass the number of MACs to the Ethos-U55 model use the `ethosu.num_macs` parameter.\n",
    "If the number of MACs used in the compilation does not match the model configuration at runtime, the inference will fail\n",
    "with an NPU config mismatch error. It is essential to check that the number of MACs is the same for the build and\n",
    "for the run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da01ca7",
   "metadata": {},
   "source": [
    "- Ethos-U model capable of producing cycle approximate results (within 10% tolerance).\n",
    "- Cannot be used to profile Cortex-M55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "FVP_Corstone_SSE-300_Ethos-U55 -C ethosu.num_macs=128 \\\n",
    "    -C mps3_board.telnetterminal0.start_telnet=0 \\\n",
    "    -C mps3_board.uart0.out_file='-' \\\n",
    "    -C mps3_board.uart0.shutdown_on_eot=1 \\\n",
    "    -C mps3_board.visualisation.disable-visualisation=1 \\\n",
    "    --stat /home/ubuntu/projects/MicroSpeechEthosU55/ml-embedded-evaluation-kit/build/bin/ethos-u-inference_runner.axf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae527735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
